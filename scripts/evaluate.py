import config
import os
import sys
import numpy as np
from stable_baselines3 import PPO
import argparse
import time
import csv
import json
import datetime
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.f1_env import F1TenthRL


def main():
    parser = argparse.ArgumentParser(description='F1Tenth Model Benchmark Evaluator')
    parser.add_argument('--episodes', type=int, default=10, help='è©•ä¾¡ã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
    parser.add_argument('--max_steps', type=int, default=2000, help='1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚ãŸã‚Šã®æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°')
    parser.add_argument('--model', type=str, default=None, help='ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹(æ‹¡å¼µå­ãªã—)')
    args = parser.parse_args()

    # ç’°å¢ƒã®åˆæœŸåŒ–
    env = F1TenthRL(config.MAP_PATH)
    print(f"ç¾åœ¨ã®è¦³æ¸¬ç©ºé–“ã®å½¢çŠ¶: {env.observation_space.shape}")

    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    target_model = args.model if args.model else config.MODEL_PATH
    if not target_model.endswith('.zip'):
        target_model += '.zip'

    if os.path.exists(target_model):
        try:
            model = PPO.load(target_model, device=config.DEVICE)
            print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {target_model}")
        except ValueError as e:
            print("--- èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ---")
            print(f"ãƒ¢ãƒ‡ãƒ« '{target_model}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            print("è¦³æ¸¬ç©ºé–“ã®æ¬¡å…ƒè¨­å®šï¼ˆLIDAR_DOWNSAMPLE_FACTOR ç­‰ï¼‰ãŒå­¦ç¿’æ™‚ã¨ç•°ãªã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            print(f"è©³ç´°: {e}")
            return
    else:
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {target_model}")
        return

    print(f"\n--- ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹ ({args.episodes} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰) ---")

    results = {
        "steps": [],
        "rewards": [],
        "avg_speeds": [],
        "collisions": 0,
        "success": 0
    }
    statuses = []

    start_time = time.time()

    for ep in range(args.episodes):
        obs = env.reset()
        done = False
        ep_reward = 0
        ep_steps = 0
        speeds = []

        while not done and ep_steps < args.max_steps:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)

            try:
                speed = env.env.sim.agents[0].state[3]
                speeds.append(speed)
            except:
                pass

            ep_reward += reward
            ep_steps += 1

        results["steps"].append(ep_steps)
        results["rewards"].append(ep_reward)
        results["avg_speeds"].append(np.mean(speeds) if speeds else 0)

        # æˆåŠŸ/è¡çªã®åˆ¤å®š
        # F1Tenth gym ã§ã¯ done=True ãŒè¡çªï¼ˆå£æ¥è§¦ã«ã‚ˆã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ï¼‰ã‚’æ„å‘³ã™ã‚‹
        # done=False ã®ã¾ã¾ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ãŸå ´åˆã¯æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°åˆ°é”ï¼ˆå®Œèµ°ï¼‰
        if done:
            results["collisions"] += 1
            status = "Collision"
        else:
            results["success"] += 1
            status = "Success (Max Steps)"

        statuses.append(status)
        print(f"Episode {ep+1:02d}: Steps={ep_steps:4d}, Reward={ep_reward:7.1f}, Speed={np.mean(speeds):.2f}m/s, {status}")

    total_time = time.time() - start_time

    print("\n" + "="*40)
    print("ğŸ“Š æœ€çµ‚ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ")
    print("="*40)
    print(f"ãƒ¢ãƒ‡ãƒ«: {os.path.basename(target_model)}")
    print(f"ç·è¨ˆæ™‚é–“: {total_time:.2f} ç§’")
    print(f"æˆåŠŸç‡ (å®Œèµ°): {results['success'] / args.episodes * 100:.1f}%")
    print(f"è¡çªç‡: {results['collisions'] / args.episodes * 100:.1f}%")
    print("-"*40)
    print(f"å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°: {np.mean(results['steps']):.1f} steps")
    print(f"å¹³å‡ç´¯ç©å ±é…¬: {np.mean(results['rewards']):.1f}")
    print(f"å…¨ä½“å¹³å‡é€Ÿåº¦: {np.mean(results['avg_speeds']):.2f} m/s")
    print(f"æœ€é«˜å¹³å‡é€Ÿåº¦: {np.max(results['avg_speeds']):.2f} m/s")
    print("="*40)

    # çµæœã‚’ CSV ã¨ JSON ã«ä¿å­˜
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "logs")
    os.makedirs(log_dir, exist_ok=True)
    csv_path  = os.path.join(log_dir, f"benchmark_{timestamp}.csv")
    json_path = os.path.join(log_dir, f"benchmark_{timestamp}.json")

    with open(csv_path, "w", newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["episode", "steps", "reward", "avg_speed", "status"])
        for i in range(args.episodes):
            writer.writerow([i+1, results["steps"][i], results["rewards"][i], results["avg_speeds"][i], statuses[i]])

    with open(json_path, "w") as jsonfile:
        json.dump({
            "model": os.path.basename(target_model),
            "episodes": args.episodes,
            "max_steps": args.max_steps,
            "total_time_sec": total_time,
            "success_rate": results['success'] / args.episodes,
            "collision_rate": results['collisions'] / args.episodes,
            "avg_steps": float(np.mean(results['steps'])),
            "avg_reward": float(np.mean(results['rewards'])),
            "avg_speed": float(np.mean(results['avg_speeds'])),
            "per_episode": [
                {"episode": i+1, "steps": results["steps"][i],
                 "reward": results["rewards"][i],
                 "avg_speed": results["avg_speeds"][i],
                 "status": statuses[i]}
                for i in range(args.episodes)
            ]
        }, jsonfile, indent=2, ensure_ascii=False)

    print(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}")
    print(f"            ã¨: {json_path}")


if __name__ == '__main__':
    main()
